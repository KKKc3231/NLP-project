{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as  plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"文件名.xlsx\",engine='openpyxl')[[\"字段1\",\"字段n\"]]\n",
    "df.head(),df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['字段筛选']=df['原字段'].apply(lambda x:1 if x>30 else 0)           #筛选数据\n",
    "df=df.drop_duplicates() ## 去重\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.concat([df[['字段n']],df[['字段n']],df[['字段n']]])  #数据复制\n",
    "y=pd.concat([df.label,df.label,df.label])           #数据扩充机制，加入标签\n",
    "X.columns=['字段n']\n",
    "X.reset_index\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba                                        #分词机制，有词表加入词表\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "X['cuted_term']=X[\"term\"].apply(chinese_word_cut)     #拿到分词结果\n",
    "X['cuted_term'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['cuted_term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "X_train,X_test,y_train,y_test= train_test_split(X,y,random_state=42,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_stopwords(stop_words_file):                          #有停用词加载停用词\n",
    "    with open(stop_words_file,encoding=\"utf-8\") as f:\n",
    "        custom_stopwords_list=[i.strip() for i in f.readlines()]\n",
    "    return custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = \"stopwords.txt\"\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "stopwords[-10:]        #查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "vect=CountVectorizer()   #实例化\n",
    "vect      #查看参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"cuted_term\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train[\"cuted_term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit_transform(X_train[\"cuted_term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit_transform(X_train[\"cuted_term\"]).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(vect.fit_transform(X_train[\"cuted_term\"]).toarray(),columns=vect.get_feature_names()).iloc[:10,:22]\n",
    "# print(vect.get_feature_names())\n",
    "# #  数据维数1956，不算很大（未使用停用词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',stop_words=frozenset(stopwords)) # 去除停用词\n",
    "pd.DataFrame(vect.fit_transform(X_train['cuted_term']).toarray(), columns=vect.get_feature_names()).head()\n",
    "# 1691 columns,去掉以数字为特征值的列，减少了三列编程1691 \n",
    "# max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "# min_df = 3 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import  metrics\n",
    "svc_cl=SVC()\n",
    "pipe=make_pipeline(vect,svc_cl)\n",
    "pipe.fit(X_train.cuted_term, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test.cuted_term)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(conf,clas):                 #混淆矩阵\n",
    "    import  matplotlib.pyplot as  plt\n",
    "    fig,ax=plt.subplots(figsize=(2.5,2.5))\n",
    "    ax.matshow(conf,cmap=plt.cm.Blues,alpha=0.3)\n",
    "    tick_marks = np.arange(len(clas))\n",
    "    plt.xticks(tick_marks,clas, rotation=45)\n",
    "    plt.yticks(tick_marks, clas)\n",
    "    for i in range(conf.shape[0]):\n",
    "        for j in range(conf.shape[1]):\n",
    "            ax.text(x=i,y=j,s=conf[i,j],\n",
    "                   va='center',\n",
    "                   ha='center')\n",
    "    plt.xlabel(\"predict_label\")\n",
    "    plt.ylabel(\"true label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=metrics.confusion_matrix(y_test,y_pred)\n",
    "class_names=np.array(['0','1'])\n",
    "get_confusion_matrix(np.array(conf),clas=class_names)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对整个数据集进行预测分类\n",
    "y_pred_all = pipe.predict(X['cuted_term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y,y_pred_all)\n",
    "# 对于整个样本集的预测正确率，整个数据集的准确率高于测试集，说明有些过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y,y_pred_all)\n",
    "#  真个数据集的混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()\n",
    "# 初始样本中 正类与负类的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_true=y,y_pred=y_pred_all)\n",
    "# f1_score 评价模型对于真个数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(y, y_pred_all)\n",
    "# 检出率，也就是正类总样本检出的比例   真正/假阴+真正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(y, y_pred_all)\n",
    "#  准确率，  检测出的来正类中真正类的比例  真正/假阳+真正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y, y_pred_all))\n",
    "# 分类报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  支持向量机分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_cl=SVC() # 实例化\n",
    "pipe=make_pipeline(vect,svc_cl)\n",
    "pipe.fit(X_train.cuted_term, y_train)\n",
    "y_pred = pipe.predict(X_test.cuted_term)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 支持向量机 网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import  Pipeline\n",
    "# svc=SVC(random_state=1)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf=TfidfTransformer()\n",
    "# ('tfidf',\n",
    "#                       TfidfTransformer()),\n",
    "#                      ('clf',\n",
    "#                       SGDClassifier(max_iter=1000)),\n",
    "# svc=SGDClassifier(max_iter=1000)\n",
    "svc=SVC()\n",
    "# pipe=make_pipeline(vect,SVC)\n",
    "pipe_svc=Pipeline([(\"scl\",vect),('tfidf',tfidf),(\"clf\",svc)])\n",
    "para_range=[0.0001,0.001,0.01,0.1,1.0,10,100,1000]\n",
    "para_grid=[\n",
    "    {'clf__C':para_range,\n",
    "    'clf__kernel':['linear']},\n",
    "    {'clf__gamma':para_range,\n",
    "    'clf__kernel':['rbf']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs=GridSearchCV(estimator=pipe_svc,param_grid=para_grid,cv=10,n_jobs=-1)\n",
    "gs=GridSearchCV(estimator=pipe_svc,param_grid=para_grid,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.cuted_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train.cuted_term,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_.fit(X_train.cuted_term,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_test.cuted_term)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 临近法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import  KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=4,p=2,metric='minkowski')\n",
    "pipe=make_pipeline(vect,knn)\n",
    "pipe.fit(X_train.cuted_term, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test.cuted_term)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree=DecisionTreeClassifier(criterion='entropy',random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=make_pipeline(vect,tree)\n",
    "pipe.fit(X_train.cuted_term, y_train)\n",
    "y_pred = pipe.predict(X_test.cuted_term)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest=RandomForestClassifier(criterion='entropy',random_state=1,n_jobs=2)\n",
    "pipe=make_pipeline(vect,forest)\n",
    "pipe.fit(X_train.cuted_term, y_train)\n",
    "y_pred = pipe.predict(X_test.cuted_term)\n",
    "metrics.accuracy_score(y_test,y_pred)\n",
    "# 加上tfidf反而准确率96.5至95.0，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bagging方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag=BaggingClassifier(base_estimator=tree,\n",
    "                     n_estimators=10,\n",
    "                     max_samples=1.0,\n",
    "                     max_features=1.0,\n",
    "                     bootstrap=True,\n",
    "                     bootstrap_features=False,\n",
    "                     n_jobs=1,random_state=1)\n",
    "pipe=make_pipeline(vect,tfidf,bag)\n",
    "pipe.fit(X_train.cuted_term, y_train)\n",
    "y_pred = pipe.predict(X_test.cuted_term)\n",
    "metrics.accuracy_score(y_test,y_pred)  #  没用转化td-idf 93.2%, 加上转化步骤，准确率提升到95.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Gradient Boosting方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "grd = GradientBoostingClassifier(learning_rate=0.18,max_depth=10,n_estimators=240,random_state=42,max_features='sqrt',subsample=0.9,\n",
    "                                min_impurity_decrease=0.01)\n",
    "                                \n",
    "print(grd)\n",
    "# Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n",
    "# Choosing max_features < n_features leads to a reduction of variance and an increase in bias.降低过拟合，但是可能会增加偏差，降低方差（对应的过拟合）\n",
    "pipe=make_pipeline(vect,tfidf,grd)\n",
    "pipe.fit(X_train.cuted_term, y_train)\n",
    "y_pred = pipe.predict(X_test.cuted_term)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost 方法 sklearn类似 api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier   \n",
    "# sklearn API 类似于导入的从skearn中导入某个算法，然后再进行实例化即可，初始化算法的时候可以修改默认参数\n",
    "from xgboost import plot_importance\n",
    "x_train_vect=vect.fit_transform(X_train[\"cuted_term\"])\n",
    "x_test_vect= vect.transform(X_test[\"cuted_term\"])\n",
    "clf = XGBClassifier(\n",
    "silent=1 ,#设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "# #nthread=4,# cpu 线程数 默认最大\n",
    "learning_rate= 0.20, # 学习率\n",
    "min_child_weight=0.5, \n",
    "# # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "# #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "# #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "gamma=0.1,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "subsample=0.7, # 随机采样训练样本 训练实例的子采样比\n",
    "max_depth=15,\n",
    "max_delta_step=0,#最大增量步长，我们允许每个树的权重估计。\n",
    "colsample_bylevel=0.7, # Subsample ratio of columns for each split, in each level.\n",
    "colsample_bytree=0.6, # 生成树时进行的列采样 \n",
    "reg_lambda=0.04,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越保守\n",
    "reg_alpha=0.003, # L1 正则项参数，参数越大，模型越保守\n",
    "### 正则化是在梯度提升树种没有的，这是xgboost与GB方法的区别之一。\n",
    "scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重=sum(负类样本)/sum(正类样本)\n",
    "# objective= 'reg:logistic', #多分类的问题 指定学习任务和相应的学习目标\n",
    "objective='binary:logistic' ,\n",
    "# #num_class=10, # 类别数，多分类与 multisoftmax 并用\n",
    "n_estimators=1200, #树的个数\n",
    "random_state=42\n",
    "# #eval_metric= 'auc'\n",
    ")\n",
    "evals  = [(x_test_vect, y_test)]\n",
    "clf.fit(x_train_vect,y_train,eval_set=evals,early_stopping_rounds=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(x_test_vect)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost learning  API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vect=vect.fit_transform(X_train[\"cuted_term\"])\n",
    "x_test_vect= vect.transform(X_test[\"cuted_term\"])\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(x_train_vect, label=y_train)\n",
    "dtest=xgb.DMatrix(x_test_vect,label=y_test)\n",
    "evals  = [(dtest, 'eval')]\n",
    "param = {'max_depth':15, 'eta':0.2, 'silent':1, 'objective':'binary:logistic', 'gamma':0.1,'colsample_bytree':0.6,\n",
    "        'alpha':0.003,'lambda':0.04, 'colsample_bylevel':0.7,'subsample':0.7,'min_child_weight':0.5,'evals':evals,'early_stopping_rounds':200 }\n",
    "\n",
    "bst = xgb.train(param, dtrain,1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bst.best_ntree_limit)\n",
    "y_pred = bst.predict(dtest,ntree_limit=bst.best_ntree_limit)\n",
    "y_pred=[1 if i>0.5 else 0 for i in y_pred]\n",
    "# predictions = [round(value) for value in y_pred]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "194px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
